{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "89a06babd7cd4c16b24fb5588df04b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03c6028cc76f4ce58e0025e7e18032c9",
              "IPY_MODEL_28ef84ac758c4fca8496ac28bcb5ed1b",
              "IPY_MODEL_fdda11bd5e7644db95e75cbff1584616"
            ],
            "layout": "IPY_MODEL_0671c2949e4a4a6e8de6258c11b8c1be"
          }
        },
        "03c6028cc76f4ce58e0025e7e18032c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac209415e97a4a788994344595c9507a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ab8fcc677fbd436e8e472df9b001717c",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "28ef84ac758c4fca8496ac28bcb5ed1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bc7a59834154ab5a81006da6cbbd302",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b9a9d3371974f9a8ecd8a6c9f2f0f11",
            "value": 2
          }
        },
        "fdda11bd5e7644db95e75cbff1584616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_532da6ea15094293ac0343419c9ad290",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_86d54327ac7d401498b63b4ac30f4d9d",
            "value": "â€‡2/2â€‡[00:56&lt;00:00,â€‡25.76s/it]"
          }
        },
        "0671c2949e4a4a6e8de6258c11b8c1be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac209415e97a4a788994344595c9507a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab8fcc677fbd436e8e472df9b001717c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bc7a59834154ab5a81006da6cbbd302": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b9a9d3371974f9a8ecd8a6c9f2f0f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "532da6ea15094293ac0343419c9ad290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86d54327ac7d401498b63b4ac30f4d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## QLoRa Finetuning of LLama-2-2b on the RAFT generated dataset"
      ],
      "metadata": {
        "id": "d7nzHeaJfiyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fine-tuning process was implemented using QLoRA for memory-efficient training on the RAFT dataset. Using 4-bit quantization and LoRA adapters allowed for fine-tuning LLaMA-2-7B despite GPU memory constraints.\n",
        "\n",
        "The training implementation and hyperparameters were informed by the QLoRA paper's recommendations."
      ],
      "metadata": {
        "id": "XggSgkLefzGf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pjGb-JL_PXIA",
        "outputId": "1d925670-80d5-48a5-f2df-b53f4ac3e3d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting peft\n",
            "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Collecting trl\n",
            "  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Collecting tyro>=0.5.11 (from trl)\n",
            "  Downloading tyro-0.8.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.8.1)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.11-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: xxhash, shtab, dill, multiprocess, tyro, bitsandbytes, peft, datasets, trl\n",
            "Successfully installed bitsandbytes-0.44.1 datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 peft-0.13.0 shtab-1.7.1 trl-0.11.1 tyro-0.8.11 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision datasets transformers tokenizers bitsandbytes peft accelerate trl\n",
        "!pip install flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "ckySJPz_SILX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see: https://huggingface.co/docs/hub/security-tokens\n",
        "# must be write token to push model later\n",
        "hf_token = \"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\"\n",
        "\n",
        "# https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# name for output model\n",
        "target_model = \"ijuliet/Llama-2-7b-chat-hf-mental-health\""
      ],
      "metadata": {
        "id": "dNe8ihTrSVfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_prompt():\n",
        "    return \"\"\"\n",
        "    You are a knowledgeable and supportive psychologist. You provide emphatic, non-judgmental responses to users seeking\n",
        "    emotional and psychological support. Provide a safe space for users to share and reflect, focus on empathy, active\n",
        "    listening and understanding.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "XriJvPUFUtor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(input_dict):\n",
        "    \"\"\"\n",
        "    Preprocess the input dictionary to be in the required format.\n",
        "\n",
        "    Args:\n",
        "    input_dict (dict): The input dictionary to be preprocessed\n",
        "\n",
        "    Returns:\n",
        "    str: The preprocessed text in the required format\n",
        "    \"\"\"\n",
        "    # Extract messages from the input dictionary\n",
        "    messages = input_dict['messages']\n",
        "\n",
        "    # Extract the system message\n",
        "    system_message = next(msg['content'] for msg in messages if msg['role'] == 'system')\n",
        "\n",
        "    # Extract the user message\n",
        "    user_message = next(msg['content'] for msg in messages if msg['role'] == 'user')\n",
        "\n",
        "    # Extract the assistant message\n",
        "    assistant_message = next(msg['content'] for msg in messages if msg['role'] == 'assistant')\n",
        "\n",
        "    # Construct the output in the required format\n",
        "    output = f\"### System: {system_message}\\n\\n### User: {user_message}\\n\\n### Assistant: {assistant_message}\"\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "4NBojC9oUzEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./output.jsonl', 'r') as json_file:\n",
        "    dataset = list(json_file)"
      ],
      "metadata": {
        "id": "01b72JJymFkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.loads(dataset[400]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GixZjfk8l5B1",
        "outputId": "7590802c-a527-48cf-c8c1-3ed38c478e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [{'content': 'You are a knowledgeable and supportive psychologist. You provide emphatic, non-judgmental responses to users seeking\\n    emotional and psychological support. Provide a safe space for users to share and reflect, focus on empathy, active\\n    listening and understanding', 'role': 'system'}, {'content': '<DOCUMENT>Iâ€™m ready to let you go. BOX 14.1\\n What the Professor Really Means\\nSchismogenesis: A term coined by Deborah Tannen \\nsuggesting that exaggerated conversation styles become intensiï¬  ed under stress, thus adding to miscommunication. Metamessages: The underlying intention of verbal \\ncommunication when people are indirect with their comments, thus adding to miscommunication.Reprinted by permission of J.</DOCUMENT>\\n<DOCUMENT>The gratitude showed; the sparkle in her eyes said it all. Behavior Modiï¬  cation\\n223\\n56147_CH09_216_228.indd   22356147_CH09_216_228.indd   223 9/29/08   11:06:18 PM9/29/08   11:06:18 PMother in times of need. Over time, this ethic has \\nbecome warped so that individuals put other peo-pleâ€™s needs before their own (e.g., codependent personality). Saying no is mistakenly equated with rudeness, and doing so results in feelings of rejec-tion in the other person. But saying yes when it is inconvenient or impossible results in resentment and victimization in oneself. Assertive ness train-ing teaches people to say no without feeling guilty about hurting someone elseâ€™s feelings. People have the right to refuse a request without harboring feelings of guilt.</DOCUMENT>\\n<DOCUMENT>That will depend on how anxi ous you are\\nand how intensely youâ€™re exerting yourself. Listen to your bo dy and stop\\nwhen you feel yourself calming down. Channel your anxious ener gy into something useful\\nSometimes you â€™ll get really anxious and worked up, and even if you\\nultimately handl e it well, you may still feel amped up and on edge for a\\nlittle while. Getting nervous triggers chemicals in your body and puts it into\\nan alert state; sometimes it takes  a while for all systems to return  to normal. Some people take the attitude of , â€œW ell, if I have all this extra ener gy , I may\\nas w ell u se it,â€ and then clean t heir bathroom or or ganize their apartmentâ€™ s\\nstorage locker . Coping with nerves befor e an upcoming social event\\nAnticipating an upcoming socia l situation can cause you to feel anxious. The type s that tend to be the scariest are the ones you know you canâ€™ t back\\nout of.</DOCUMENT>\\n3. What is the purpose of assertiveness training in this context?', 'role': 'user'}, {'content': 'To answer the question about the purpose of assertiveness training in this context, we need to analyze the provided information. \\n\\n1. **Identify the issue**: The context highlights how individuals tend to prioritize other people\\'s needs over their own, leading to feelings of resentment and victimization when they agree to things they cannot or should not do.\\n\\n2. **Purpose of assertiveness training**: The purpose of assertiveness training in this context is to empower individuals to set boundaries and prioritize their own needs without feeling guilty. It teaches people how to say no assertively without sacrificing their own well-being or feeling obligated to please others at the expense of their own happiness.\\n\\n3. **Key points from the context**:\\n   - \"Assertiveness training teaches people to say no without feeling guilty about hurting someone elseâ€™s feelings.\"\\n   - \"People have the right to refuse a request without harboring feelings of guilt.\"\\n\\nTherefore, **the purpose of assertiveness training in this context is to help individuals assert their boundaries, prioritize their own needs, and say no without feeling guilty or sacrificing their well-being**.\\n\\n<ANSWER>: The purpose of assertiveness training in this context is to help individuals assert their boundaries, prioritize their own needs, and say no without feeling guilty or sacrificing their well-being.', 'role': 'assistant'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset_to_jsonl(input_dataset, output_file):\n",
        "    \"\"\"\n",
        "    Preprocess the entire dataset and save it to a JSONL file.\n",
        "\n",
        "    Args:\n",
        "    input_dataset (list): List of JSON strings, each representing a datapoint\n",
        "    output_file (str): Path to the output JSONL file\n",
        "    \"\"\"\n",
        "    with open(output_file, 'w') as f:\n",
        "        for datapoint_str in tqdm(input_dataset, desc=\"Preprocessing dataset\"):\n",
        "            try:\n",
        "                datapoint = json.loads(datapoint_str)\n",
        "                preprocessed_text = preprocess_text(datapoint)\n",
        "                json_string = json.dumps(preprocessed_text)\n",
        "                f.write(json_string + '\\n')\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Error decoding JSON: {datapoint_str[:100]}...\")  # Print first 100 chars of problematic string\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error: {e} for input: {datapoint_str[:100]}...\")\n"
      ],
      "metadata": {
        "id": "6NClJtX5uuKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_dataset_to_jsonl(dataset, 'processed_outputs.jsonl')\n",
        "\n",
        "print(\"Dataset preprocessing complete. Output saved to processed_outputs.jsonl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wbd7C6GcuyLg",
        "outputId": "3fdf7e25-5d56-4f04-ae48-03ef1a571e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6034/6034 [00:00<00:00, 10134.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset preprocessing complete. Output saved to processed_outputs.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./processed_outputs.jsonl', 'r') as json_file:\n",
        "    dataset2 = list(json_file)"
      ],
      "metadata": {
        "id": "UcJsbWT4w41u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.loads(dataset2[40]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1R3cV2fxAOX",
        "outputId": "83397fec-6b0b-40c2-c9fe-d4156522abf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### System: You are a knowledgeable and supportive psychologist. You provide emphatic, non-judgmental responses to users seeking\n",
            "    emotional and psychological support. Provide a safe space for users to share and reflect, focus on empathy, active\n",
            "    listening and understanding\n",
            "\n",
            "### User: <DOCUMENT>Wrong. Multi-tasking\tactually\tsacrifices\tyour\tquality\tof\twork,\tas\tthe\tbrain\tis\tsimply\n",
            "incapable\tof\tperforming\tat\ta\thigh\tlevel\tin\tmultiple\tactivities\tat\tonce. Letâ€™s\tsay\tyouâ€™re\tin\ta\tmeeting\twhere\tseveral\tideas\tare\tbeing\tshared.</DOCUMENT>\n",
            "<DOCUMENT>Maybe one or two coworkers arenâ€™ t fans of\n",
            "yours, but most are probably pretty neutral about you. â€œIf I go out to the bar with my friends, I know all kinds of annoying\n",
            "things will go wr ong with the night.â€ (Fortune-telling)\n",
            "Alternative:  Soc ial events hardly ever turn  out exactly as we predict or\n",
            "anticipate, good or bad. The more social experience you get, the more this\n",
            "point will be driven home. â€œI canâ€™t see myself becoming extr emely charismatic so I donâ€™t see the\n",
            "point in working on my people skills.â€ (Black-and-white thinking)\n",
            "Alternative:  Even tweaking your social sk ills a little can make a big\n",
            "dif ference in the quality of your life. Y ou only need average peo ple skills toenjoy most of what the social world has to of fer . â€œNot everyone in my class likes me. That means Iâ€™m a complete r eject.â€\n",
            "(Black-and-white thinking)\n",
            "Alternative:  Y our worth as a person doesnâ€™ t hinge on having every last\n",
            "person like you. No one is universally liked.</DOCUMENT>\n",
            "<DOCUMENT>Accept that you may not be fu lly r eady to change yet. Regardless\n",
            "of h ow you thin k you should  f eel , your heart  may not fully be in it at\n",
            "the mom ent. If you donâ€™ t have an inner drive to tackle your issue s, no\n",
            "rah-rah speech or quote is going  to fix that. At best thatâ€™ll mak e you\n",
            "feel psyched up for a day or two  before you go back to the status  quo. Thereâ€™ s nothing wrong with deciding to wait until a greater sense of\n",
            "ur gency sets in. Set aside some time to figur e out what you r eally want. Are you\n",
            "telling y ourself you should try to become an outgoing party animal\n",
            "because thatâ€™ s what society say s is important, when deep do wn it\n",
            "doesnâ€™ t interest you? W ould yo u feel more enthusiastic about trying\n",
            "to develop a more low-key social life? Realize the hardest part is often getting started.</DOCUMENT>\n",
            "1. Can social events always turn out as we predict or anticipate?\n",
            "\n",
            "### Assistant: To answer the question \"Can social events always turn out as we predict or anticipate?\" based on the provided context, we can follow these steps:\n",
            "\n",
            "1. Acknowledge the perspective presented in the context: Social events hardly ever turn out exactly as we predict or anticipate, good or bad. This is supported by the statement, \"Even tweaking your social skills a little can make a big difference in the quality of your life.\"\n",
            "\n",
            "2. Highlight the unpredictability of social events: The more social experience you gain, the more you realize that social events can be unpredictable. This is emphasized by the statement, \"The more this point will be driven home.\"\n",
            "\n",
            "3. Emphasize the variability in social interactions: Not everyone will have the same opinion or reaction in social settings. As mentioned, \"No one is universally liked.\"\n",
            "\n",
            "4. Final Answer: <ANSWER> Social events cannot always turn out as we predict or anticipate. The unpredictability and variability of social interactions make it impossible to foresee every outcome.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mental_health_model():\n",
        "    # Check if CUDA is available and the GPU is compatible with FlashAttention\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        if not any(x in gpu_name for x in [\"A100\", \"RTX 30\", \"RTX 40\", \"H100\"]):  # Check for Ampere or newer GPUs\n",
        "            print(f\"Warning: Your GPU ({gpu_name}) might not be fully compatible with FlashAttention. \"\n",
        "                  f\"Consider disabling FlashAttention for optimal performance.\")\n",
        "            attn_implementation = None  # Disable FlashAttention\n",
        "        else:\n",
        "            attn_implementation = \"flash_attention_2\"  # Enable FlashAttention\n",
        "    else:\n",
        "        attn_implementation = None  # Disable FlashAttention if no CUDA is available\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        token=hf_token,\n",
        "        quantization_config=BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=False\n",
        "        ),\n",
        "        torch_dtype=torch.float16,  # reduce memory usage\n",
        "        attn_implementation=attn_implementation  # optimize for tensor cores (NVIDIA A100)\n",
        "    )\n",
        "\n",
        "    # LoRA config based on QLoRA paper\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        r=8,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=target_model,  # model output directory\n",
        "        overwrite_output_dir=True,  # overwrite output if exists\n",
        "        num_train_epochs=2,  # number of epochs to train 3 to 5 epochs\n",
        "        per_device_train_batch_size=2,  # batch size per device during training\n",
        "        gradient_checkpointing=True,  # save memory but causes slower training\n",
        "        logging_steps=10,  # log every 10 steps\n",
        "        learning_rate=1e-4,  # learning rate\n",
        "        max_grad_norm=0.3,  # max gradient norm based on QLoRA paper\n",
        "        warmup_ratio=0.03,  # warmup ratio based on QLoRA paper\n",
        "        optim=\"paged_adamw_8bit\",  # memory-efficient variant of AdamW optimizer\n",
        "        lr_scheduler_type=\"constant\",  # constant learning rate\n",
        "        save_strategy=\"epoch\",  # save at the end of each epoch\n",
        "        evaluation_strategy=\"epoch\",  # evaluation at the end of each epoch,\n",
        "        fp16=True,  # use fp16 16-bitprecision training instead of 32-bit to save memory\n",
        "        #tf32=True  # optimize for tensor cores (NVIDIA A100)\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model, token=hf_token)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # limit samples to reduce memory usage\n",
        "    dataset = load_dataset(\"json\", data_files=\"output.jsonl\", split=\"train\")\n",
        "    train_dataset = dataset.select(range(2000))\n",
        "    eval_dataset = dataset.select(range(2000, 2500))\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        peft_config=peft_config,\n",
        "        max_seq_length=1024,\n",
        "        tokenizer=tokenizer,\n",
        "        packing=True,\n",
        "        args=args\n",
        "    )\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n",
        "    trainer.push_to_hub(target_model, token=hf_token)\n"
      ],
      "metadata": {
        "id": "tiJh0DhLUzzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mental_health_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779,
          "referenced_widgets": [
            "89a06babd7cd4c16b24fb5588df04b5c",
            "03c6028cc76f4ce58e0025e7e18032c9",
            "28ef84ac758c4fca8496ac28bcb5ed1b",
            "fdda11bd5e7644db95e75cbff1584616",
            "0671c2949e4a4a6e8de6258c11b8c1be",
            "ac209415e97a4a788994344595c9507a",
            "ab8fcc677fbd436e8e472df9b001717c",
            "3bc7a59834154ab5a81006da6cbbd302",
            "4b9a9d3371974f9a8ecd8a6c9f2f0f11",
            "532da6ea15094293ac0343419c9ad290",
            "86d54327ac7d401498b63b4ac30f4d9d"
          ]
        },
        "id": "Qmo4F9GBmQUc",
        "outputId": "39b24686-5623-4a5c-ffe6-4987c2cd1998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your GPU (Tesla T4) might not be fully compatible with FlashAttention. Consider disabling FlashAttention for optimal performance.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89a06babd7cd4c16b24fb5588df04b5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='2458' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   7/2458 00:30 < 4:06:58, 0.17 it/s, Epoch 0.00/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8dac5ae6e2cb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_mental_health_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-e3df4b1f8c78>\u001b[0m in \u001b[0;36mtrain_mental_health_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trl_activate_neftune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# After training we make sure to retrieve back the original forward pass method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2282\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2283\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2284\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2285\u001b[0m                 ):\n\u001b[1;32m   2286\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}